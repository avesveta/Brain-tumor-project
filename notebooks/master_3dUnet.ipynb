{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Dense, \\\n",
    "    GlobalAveragePooling3D, BatchNormalization, Dropout, \\\n",
    "        UpSampling3D, concatenate, Flatten, ConvLSTM2D, Bidirectional, \\\n",
    "            Cropping3D, ZeroPadding3D, Activation, Input, UpSampling3D, \\\n",
    "                Conv3DTranspose\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 137\n",
    "# using the range from segmentation\n",
    "# [40:195,35:225,26:137]\n",
    "# constants to crop images\n",
    "\n",
    "MIN_WIDTH = 25 # 40\n",
    "MAX_WIDTH = 215 # 195\n",
    "\n",
    "MIN_HEIGHT = 35 # 35\n",
    "MAX_HEIGHT = 225 # 225\n",
    "\n",
    "MIN_DEPTH = 15 # 26\n",
    "MAX_DEPTH = 143 # 137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel \n",
    "channel = 't1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_pickle(f\"../../raw_data/Grade_ID_{channel}_nii.pkl\")\n",
    "\n",
    "# the target columns have to be updated, it's only the fremework\n",
    "df['Grade'] = df['Grade'].apply(lambda x: 1 if x == 'HGG' else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {df[f'{channel}_nii'].shape[0]}\")\n",
    "print(f\"Negative examples: {df[df['Grade'] == 0].shape}\")\n",
    "print(f\"Positive examples: {df[df['Grade'] == 1].shape}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Single example Shape:\")\n",
    "print(f\"Height: {df[f'{channel}_nii'][0].shape[0]}\")\n",
    "print(f\"Width: {df[f'{channel}_nii'][0].shape[1]}\")\n",
    "print(f\"Depth: {df[f'{channel}_nii'][0].shape[2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Crop Images and run Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop images\n",
    "df[f'{channel}_nii'] = df[f'{channel}_nii'].apply(lambda x: np.array(x[MIN_HEIGHT:MAX_HEIGHT,MIN_WIDTH:MAX_WIDTH,MIN_DEPTH:MAX_DEPTH]))\n",
    "X = df[f'{channel}_nii']\n",
    "y = df['Grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input data\n",
    "X = np.array([np.array(val) for val in X])\n",
    "if channel != 'seg':\n",
    "    X[X>840]=840 # set the max value to 840\n",
    "    X = (X - 0)/840 # Min-Max Scaler\n",
    "#reshape the X to fit the input of Model\n",
    "X = X.reshape(len(X), X[0].shape[0], X[0].shape[1], X[0].shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=SEED)\n",
    "# reset the index of y, so that the indexes of X and y match\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the not used variables and release RAM\n",
    "del [df,X]\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices of positive examples\n",
    "pos_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "# perform data augmentation on positive examples\n",
    "rotated_examples1 = np.rot90(X_train[pos_indices], axes=(1, 2))\n",
    "rotated_examples2 = np.rot90(rotated_examples1, axes=(1, 2))\n",
    "\n",
    "# append new images and labels to the training set\n",
    "X_train = np.concatenate([X_train, rotated_examples1, rotated_examples2], axis=0)\n",
    "y_train = np.concatenate([y_train, np.repeat(y_train[pos_indices], 2)], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sasha's but with sigmoid\n",
    "\n",
    "in_layer = Input((194, 186, 128, 1))\n",
    "bn = BatchNormalization()(in_layer)\n",
    "cn1 = Conv3D(8, \n",
    "             kernel_size = (1, 5, 5), \n",
    "             padding = 'same',\n",
    "             activation = 'relu')(bn)\n",
    "bn2 = Activation('relu')(BatchNormalization()(cn1))\n",
    "\n",
    "dn1 = MaxPooling3D((2, 2, 2))(bn2)\n",
    "cn3 = Conv3D(16, \n",
    "             kernel_size = (3, 3, 3),\n",
    "             padding = 'same',\n",
    "             activation = 'relu')(dn1)\n",
    "bn3 = Activation('relu')(BatchNormalization()(cn3))\n",
    "\n",
    "dn2 = MaxPooling3D((1, 2, 2))(bn3)\n",
    "cn4 = Conv3D(32, \n",
    "             kernel_size = (3, 3, 3),\n",
    "             padding = 'same',\n",
    "             activation = 'relu')(dn2)\n",
    "bn4 = Activation('relu')(BatchNormalization()(cn4))\n",
    "\n",
    "up1 = Conv3DTranspose(16, \n",
    "                      kernel_size = (3, 3, 3),\n",
    "                      strides = (1, 2, 2),\n",
    "                     padding = 'same')(bn4)\n",
    "\n",
    "cat1 = concatenate([up1, bn3], axis=2)\n",
    "\n",
    "up2 = Conv3DTranspose(8, \n",
    "                      kernel_size = (3, 3, 3),\n",
    "                      strides = (2, 2, 2),\n",
    "                     padding = 'same')(cat1)\n",
    "\n",
    "pre_out = concatenate([up2, bn2], axis=2)\n",
    "\n",
    "#pre_out\n",
    "pre_out = Conv3D(1, \n",
    "             kernel_size = (1, 1, 1), \n",
    "             padding = 'same',\n",
    "             activation = 'relu')(pre_out)\n",
    "\n",
    "#pre_out = Cropping3D((1, 2, 2))(pre_out) # avoid skewing boundaries\n",
    "#out = ZeroPadding3D((1, 2, 2))(pre_out)\n",
    "#pre_out = Dense(512, activation = 'relu')(pre_out)    \n",
    "\n",
    "pre_out = Flatten()(pre_out)\n",
    "\n",
    "pre_out = Dense(32, activation = 'relu')(pre_out) \n",
    "out = Dense(1, activation='sigmoid')(pre_out)\n",
    "sim_model = Model(inputs = [in_layer], outputs = [out])\n",
    "sim_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=Adam(learning_rate= 0.001)\n",
    "sim_model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = optim,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Size of X_train: {(X_train.size * X_train.itemsize) / 1e9} Gb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5, restore_best_weights = True)\n",
    "history = sim_model.fit(X_train, y_train,\n",
    "                        epochs = 30,\n",
    "                        batch_size = 4,\n",
    "                        callbacks = [es],\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        shuffle =True,\n",
    "                        verbose = 1)\n",
    "sim_model.save(f'model_glioma_{channel}_nii_3dUnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the learning curve\n",
    "def plot_loss(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=1)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax1.grid(axis=\"x\",linewidth=0.2)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.2)    \n",
    "    \n",
    "    ax2.plot(history.history['accuracy'])\n",
    "    ax2.plot(history.history['val_accuracy'])\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=1)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.grid(axis=\"x\",linewidth=0.2)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.2)    \n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (sim_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
